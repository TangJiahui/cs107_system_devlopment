
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Module 2: Deeper Into Forward Mode &#8212; Auto-eD  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Module 3: The Reverse Mode of Automatic Differentiation" href="mod3.html" />
    <link rel="prev" title="Module 1: The Basics of Forward Mode" href="mod1.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-2-deeper-into-forward-mode">
<h1>Module 2: Deeper Into Forward Mode<a class="headerlink" href="#module-2-deeper-into-forward-mode" title="Permalink to this headline">¶</a></h1>
<p>As we introduced in Module 1, the forward mode of automatic differentiation computes derviatives by decomposing functions into a series of elementary operations.  We can explicitly compute the derivative of each of these elementary operations, allowing us to combine them using the chain rule to accurately compute the derivative of our function.  As we have seen, in the computational graph, nodes represent inputs and outputs of elementary operations, and the edges correspond to the elementary operations that join these nodes.  The inputs to our functions become the first nodes in our graph.  For each subsequent node, we can consider an evaluation and derivative up to that point in the graph, allowing us to consider the computation as a series of elementary traces.</p>
<div class="section" id="the-computational-trace-and-practice-with-the-visualization-tool">
<h2>The Computational Trace and Practice with the Visualization Tool<a class="headerlink" href="#the-computational-trace-and-practice-with-the-visualization-tool" title="Permalink to this headline">¶</a></h2>
<p>At each step in the graph, we can consider the current function value and derivative up to that node.  Using the chain rule, we compute the derivative at a particular node from the elementary operation that created that node and the value and derivative of the input node to that elementary operation.  Let’s return to our example from our first demo.</p>
<div class="math notranslate nohighlight">
\[f(x) = -\exp(-2\sin^2(4x))\]</div>
<p>In module 1, we formed the corresponding computational graph.  Now let’s use that graph to write the computational table.  Each node in the table is the output of an elmentary function, whose derivative we can compute explicitly.</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 23%" />
<col style="width: 23%" />
<col style="width: 23%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Trace</p></th>
<th class="head"><p>Elementary Function</p></th>
<th class="head"><p>Current Value</p></th>
<th class="head"><p>Elementary Function Derivative</p></th>
<th class="head"><p>Derivative Evaluated at x</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_1\)</span></p></td>
<td><p>x, input</p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\pi}{16}\)</span></p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(4x_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\pi}{4}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(4\dot{x_1}\)</span></p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sin(x_2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\sqrt{2}}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cos(x_2)\dot{x_2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2\sqrt{2}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x_4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_3^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2x_3\dot{x_3}\)</span></p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_5\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-2x_4\)</span></p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(-2\dot{x_4}\)</span></p></td>
<td><p>-8</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x_6\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(exp(x_5)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{e}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(exp(x_5)\dot{x_5}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{-8}{e}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_7\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-x_6\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{-1}{e}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\dot{x_6}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{8}{e}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(x_8\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x_1 + x_7\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\pi}{16}-\frac{1}{e}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\dot{x_1}+\dot{x_7}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1+\frac{8}{e}\)</span></p></td>
</tr>
</tbody>
</table>
<p>The visualization tool which we installed in the first unit also computes the computational table.  Input the function and compare the forward mode graph to the forward model table.</p>
<p>Notice how the computational trace corresponds to the nodes on the graph and the edges linking these nodes.  Note that the choices of labels for the traces might be different than the table we wrote by hand - compare the labels for the nodes in the graph.</p>
<p>Now let’s consider an example with multiple inputs.  Observe that the computed derivative is now the gradient vector.  Instead of maintaining an evaluation trace of a scalar derivative for a single input, we instead have  a trace of the gradient for multiple inputs.</p>
<p>In the exercises in the previous module, we practiced drawing the graph for the function</p>
<div class="math notranslate nohighlight">
\[f(x,y) = \exp(-(\sin(x)-\cos(y))^2)\]</div>
<p>The graph you drew should have the same structure as this graph produced with the visualization tool (with the exception of possibly interchanging some of the labels).</p>
<img alt="_images/Mod1Ex3Sol.PNG" src="_images/Mod1Ex3Sol.PNG" />
<p>We can also use the visualization tool to see the computational table which corresponds to the graph.  Observe that the derivative in our table is now a 2 dimensional vector, corresponding to the gradient, where each component is the derivative with respect to one of our inputs.</p>
<img alt="_images/Mod2Table.PNG" src="_images/Mod2Table.PNG" />
<p>Note that computing the gradient for this multivariate function is done by assigning a seed vector to each input, where to find the gradient we use the standard basis vectors as seeds.  We’ll discuss more about what this means automatic differentiation is computing in the next section.</p>
</div>
<div class="section" id="more-theory">
<h2>More Theory<a class="headerlink" href="#more-theory" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-multivariate-chain-rule">
<h3>The Multivariate Chain Rule<a class="headerlink" href="#the-multivariate-chain-rule" title="Permalink to this headline">¶</a></h3>
<p>As we saw in the example above, we can also use automatic differentiation to find the gradient of functions with multiple inputs.  From our multivariate calculus class, recall that we can find the derivative of a function with multiple inputs also using the chain rule.  Let g(t) = h(u(t), v(t)).</p>
<div class="math notranslate nohighlight">
\[\frac{dg}{dt} = \frac{dh}{du}\frac{du}{dt} + \frac{dh}{dv}\frac{dv}{dt}\]</div>
<p>We can write this in general form as</p>
<div class="math notranslate nohighlight">
\[\nabla_x h = \sum_{i=1}^n \frac{\partial h}{\partial y_i}\nabla_x y_i\]</div>
<p>Using this formula allows us to compute the partial derivatives for each input as we saw in the evaluation trace in our multivariate example.</p>
</div>
<div class="section" id="what-does-forward-mode-compute">
<h3>What Does Forward Mode Compute?<a class="headerlink" href="#what-does-forward-mode-compute" title="Permalink to this headline">¶</a></h3>
<p>At each evaluation step, the forward mode propagates the derivative to the next node using the chain rule to evaluate the derivative from the previous node and the elementary operation.  Notice that we do not need to store all of the values at each node in memory, but instead, we only need to store value and derivative values until all of the children of a node have been evaluated.</p>
<p>If we consider the most general case, we are interested in computing Jacobians of vector valued functions of multiple variables.  To compute these individual gradients, we started our evaluation table with a seed vector, p.  This allows us to consider directional derivatives, <span class="math notranslate nohighlight">\(D_p x_k = \sum dx_3/dx_j p_j\)</span>, so we find that forward mode actually computes directional derivatives and when we choose our seed vectors to be standard unit vectors, we attain the standard gradient.  Extending this to vector valued functions, we have that forward mode computes <span class="math notranslate nohighlight">\(Jp\)</span>, a Jacobian vector product.</p>
</div>
<div class="section" id="how-efficient-is-forward-mode">
<h3>How Efficient is Forward Mode?<a class="headerlink" href="#how-efficient-is-forward-mode" title="Permalink to this headline">¶</a></h3>
<p>From this analysis of what forward mode computes, we see that the efficiency of forward mode depends on the number of input variables.  Thus, forward mode will be less efficient when we have a large number of input variables.</p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="neural-network-problem">
<h3>Neural Network Problem<a class="headerlink" href="#neural-network-problem" title="Permalink to this headline">¶</a></h3>
<p>Artificial neural networks take as input the values of an input layer of neurons and combine these inputs in a series of layers to compute an output.  A small network with a single hidden layer is drawn below.</p>
<img alt="_images/NNFigNoPhi.png" src="_images/NNFigNoPhi.png" />
<p>The network can be expressed in matrix notation as</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x,y) = w_{out}^Tz\left(W\begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix}b_1 \\ b_2 \end{bmatrix}\right)+b_{out}\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}W = \begin{bmatrix} w_{11} &amp; w_{12} \\ w_{21} &amp; w_{22}\end{bmatrix}\end{split}\]</div>
<p>is a (real) matrix of weights, and</p>
<div class="math notranslate nohighlight">
\[\begin{split}w_{out} = \begin{bmatrix}w_{out,1} \\ w_{out,2}\end{bmatrix}\end{split}\]</div>
<p>is a vector representing output weights, <span class="math notranslate nohighlight">\(b_i\)</span> are bias terms and <span class="math notranslate nohighlight">\(z\)</span> is a nonlinear function that acts component wise.</p>
<p>The above graph helps us visualize the computation in different layers.  This visualization hides many of the underlying operations which occur in the computation of <span class="math notranslate nohighlight">\(f\)</span> (e.g. it does not explicitly express the elementary operations).</p>
<p>Your tasks:
In this part, you will completely neglect the biases.  The mathematical form is therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}f(x,y) = w_{out}^Tz\left(W\begin{bmatrix}x \\ y \end{bmatrix}\right)\end{split}\]</div>
<p>Note that in practical applications the biases play a key role.  However, we have elected to neglect them in this problem so that your results are more readable.  You will complete the two steps below while neglecting the bias terms.</p>
<ol class="arabic simple">
<li><p>Draw the complete forward computational graph.  You may treat <span class="math notranslate nohighlight">\(z\)</span> as a single elementary operation.  You should explicitly show the multiplications and additions that are masked in the schematic of the network above.</p></li>
<li><p>Use your graph to write out the full forward mode table, including columns for the trace, elementary function, current function value, elementary function, derivative, partial x derivative, and partial y derivative.</p></li>
</ol>
</div>
<div class="section" id="operation-count-problem">
<h3>Operation Count Problem<a class="headerlink" href="#operation-count-problem" title="Permalink to this headline">¶</a></h3>
<p>Count the number of operations required to compute the derivatives in the neural network using forward mode.</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Auto-eD</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mod1.html">Module 1: The Basics of Forward Mode</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Module 2: Deeper Into Forward Mode</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-computational-trace-and-practice-with-the-visualization-tool">The Computational Trace and Practice with the Visualization Tool</a></li>
<li class="toctree-l2"><a class="reference internal" href="#more-theory">More Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mod3.html">Module 3: The Reverse Mode of Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mod4.html">Beyond the Basics: Extensions and Software Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="refs.html">Referernces and Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="sols.html">Solutions to Exercises</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="mod1.html" title="previous chapter">Module 1: The Basics of Forward Mode</a></li>
      <li>Next: <a href="mod3.html" title="next chapter">Module 3: The Reverse Mode of Automatic Differentiation</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Lindsey Brown and David Sondak.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/mod2.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>